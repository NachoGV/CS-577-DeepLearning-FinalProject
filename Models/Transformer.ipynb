{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "\n",
    "Model based on the following references:\n",
    "  * [Detection-Transformer](https://github.com/AarohiSingla/Detection-Transformer/tree/main)\n",
    "  * [Using Custom Datasets to train DETR for object detection](https://medium.com/@soumyajitdatta123/using-custom-datasets-to-train-detr-for-object-detection-75a6426b3f4e)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Utils to path\n",
    "import sys\n",
    "sys.path.append('../Utils')  # Adds higher directory to python modules path.\n",
    "\n",
    "# Utils\n",
    "from datasets_generators import CocoDetection\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DetrImageProcessor\n",
    "from transformers import DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "imgs_path = \"../ExDark_All/Images\"\n",
    "train_path = \"../ExDark_COCO/train_set.json\"\n",
    "val_path = \"../ExDark_COCO/val_set.json\"\n",
    "test_path = \"../ExDark_COCO/test_set.json\"\n",
    "\n",
    "# Transforms\n",
    "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# COCO Dataset\n",
    "train_set = CocoDetection(image_directory_path=imgs_path,  annotation_file_path=train_path, image_processor=image_processor)\n",
    "val_set = CocoDetection(image_directory_path=imgs_path,  annotation_file_path=val_path, image_processor=image_processor)\n",
    "test_set = CocoDetection(image_directory_path=imgs_path,  annotation_file_path=test_path, image_processor=image_processor)\n",
    "\n",
    "# Collade function\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Labels\n",
    "id2label = {k: v[\"name\"] for k, v in train_set.coco.cats.items()}\n",
    "\n",
    "# DataLoaders\n",
    "TRAIN_DATALOADER = DataLoader(dataset=train_set, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=val_set, collate_fn=collate_fn, batch_size=4)\n",
    "TEST_DATALOADER = DataLoader(dataset=test_set, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"facebook/detr-resnet-50\", \n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation/loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # DETR authors decided to use different learning rate for backbone\n",
    "        # you can learn more about it here: \n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TRAIN_DATALOADER\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return VAL_DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([13, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(TRAIN_DATALOADER))\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\ignac\\anaconda3\\envs\\dl_finalproject\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | DetrForObjectDetection | 41.5 M\n",
      "-------------------------------------------------\n",
      "41.3 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.018   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c532004a4fe469d81d9d895531aebac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ignac\\anaconda3\\envs\\dl_finalproject\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\ignac\\anaconda3\\envs\\dl_finalproject\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\ignac\\anaconda3\\envs\\dl_finalproject\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b679652479429a8e6037492a11045b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ignac\\anaconda3\\envs\\dl_finalproject\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# settings\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "trainer = Trainer(devices=1, accelerator=\"cpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
